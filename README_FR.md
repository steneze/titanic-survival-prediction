# ğŸš¢ Titanic Survival Prediction

Analyse complÃ¨te et modÃ©lisation Machine Learning pour prÃ©dire la survie des passagers du Titanic.

**Score Kaggle : 78.7% (Top 15% - Rang 2146/14636)**

![Kaggle Score](images/Kaggle_score.png)

---

## ğŸ“Š RÃ©sumÃ© du Projet

Ce projet explore le cÃ©lÃ¨bre dataset Titanic de Kaggle avec une approche mÃ©thodique :
- Analyse exploratoire complÃ¨te (EDA)
- Feature engineering avancÃ©
- Comparaison de multiples modÃ¨les
- Ensembling et optimisation

**RÃ©sultat final : 78.7% d'accuracy, classement Top 15%**

---

## ğŸ¯ Objectifs

1. Comprendre les facteurs de survie (classe, sexe, Ã¢ge, famille)
2. CrÃ©er des features pertinentes (titre, taille famille, interactions)
3. Tester et comparer plusieurs algorithmes ML
4. Optimiser via ensembling

---

## ğŸ“ Structure du Projet
```
â”œâ”€â”€ 01_EDA.ipynb              # Exploration des donnÃ©es
â”œâ”€â”€ 02_ML_Training.ipynb      # EntraÃ®nement des modÃ¨les
â”œâ”€â”€ data/                          # Datasets
â”œâ”€â”€ images/                        # Images
â”œâ”€â”€ models/                        # ModÃ¨les sauvegardÃ©s
â””â”€â”€ submissions/                   # Fichiers Kaggle

```

---

## ğŸ” Insights ClÃ©s

### 1. Le Sexe est le Facteur Principal
- **Femmes** : 74% de survie
- **Hommes** : 19% de survie
- Confirme la rÃ¨gle "Women and children first"

![Survival by Sex](images/TauxSurvie_FeatureCat.png)

### 2. La Classe Sociale Joue un RÃ´le Majeur
- **1Ã¨re classe** : 63% de survie
- **2Ã¨me classe** : 47% de survie
- **3Ã¨me classe** : 24% de survie

### 3. Interaction Sexe Ã— Classe
- **Femmes 1Ã¨re classe** : 97% de survie
- **Femmes 3Ã¨me classe** : 50% de survie
- **Hommes toutes classes** : <20% de survie

![Survival by Class and Sex](images/survival_class_sex.png)

### 4. Taille de Famille (Pattern Non-LinÃ©aire)
- **Seuls** : 30% de survie
- **Familles 2-4** : 50-60% de survie
- **Grandes familles (5+)** : 16% de survie (surtout classe 3)

---

## ğŸ› ï¸ Feature Engineering

### Features CrÃ©Ã©es

1. **Title_Simple** (6 catÃ©gories)
   - Extraction depuis `Name`
   - Regroupement des titres rares
   - Encode Ã¢ge + sexe + statut social

2. **FamilySize** = SibSp + Parch + 1

3. **HasFamily, IsAlone, Large_Family** (flags binaires)

4. **Fare_Per_Person** = Fare / FamilySize

5. **Woman_or_Child** (prioritÃ© canots)

6. **Age_Was_Missing** (flag imputation)

7. **Interactions** : Sex Ã— Pclass, Age Ã— Pclass, etc.

### Imputation des Valeurs Manquantes
- **Age** (19.9% manquant) : Moyenne par groupe (Pclass Ã— Title)
- **Cabin** (77% manquant) : Flag Has_Cabin + extraction Deck
- **Embarked** (0.2% manquant) : Mode

---

## ğŸ¤– ModÃ¨les TestÃ©s

| ModÃ¨le | Accuracy Test | ROC-AUC | Notes |
|--------|---------------|---------|-------|
| **Baseline** | 61.45% | 0.500 | MajoritÃ© simple |
| **Logistic Regression** | 84.36% | 0.876 | Avec interactions |
| **Random Forest (initial)** | 80.45% | 0.864 | Overfitting 11.5% |
| **Random Forest (tuned)** | 83.80% | 0.859 | Overfitting 2.2% âœ… |
| **XGBoost** | 81.56% | 0.846 | |
| **Gradient Boosting** | 82.5% | 0.855 | |
| **Ensemble (Voting)** | 84.1% | 0.870 | |
| **Ensemble 2 (Weighted)** | **78.7%** | - | **Kaggle Final** |

### Meilleur ModÃ¨le

**Ensemble PondÃ©rÃ©** (9 modÃ¨les)
- Random Forest, XGBoost, Gradient Boosting
- Logistic Regression avec interactions
- Voting soft/hard
- **Score Kaggle : 78.7%**

---

## ğŸ“ˆ RÃ©sultats

### Cross-Validation
- **10-Fold CV** : 82.5% Â± 2.1%
- **StratifiÃ©e** pour prÃ©server la distribution

### Kaggle Leaderboard
- **Precision Score** : 78.708%
- **Rang** : 2156 / 14684 (Top 14.7%)
- **Percentile** : Top 15%

Top 12 % after removing the 100% scores, obvious cheaters. ;-)

![Kaggle Result](images/kaggle_leaderboard.png)

---

## ğŸ“¦ Technologies UtilisÃ©es

- **Python 3.12**
- **Pandas** : Manipulation des donnÃ©es
- **NumPy** : Calculs numÃ©riques
- **Scikit-learn** : ModÃ¨les ML
- **XGBoost** : Gradient Boosting
- **Matplotlib / Seaborn** : Visualisations statiques
- **Plotly** : Visualisations interactives

---

## ğŸ“š Ressources

- [Kaggle Competition](https://www.kaggle.com/c/titanic)
- [Encyclopedia Titanica](https://www.encyclopedia-titanica.org/)
- [Scikit-learn Documentation](https://scikit-learn.org/)

---

## ğŸ“ Key Teachings

1. **Feature Engineering >> Choix du ModÃ¨le**
   - Les features crÃ©Ã©es (Title, interactions) ont plus d'impact que le choix entre RF/XGB

2. **L'Overfitting est RÃ©el**
   - CV local 82% vs Kaggle 78% = -4 points
   - RÃ©gularisation stricte nÃ©cessaire

3. **Ensembling NÃ©cessite DiversitÃ©**
   - 9 modÃ¨les similaires â†’ Pas de gain
   - NÃ©cessite modÃ¨les vraiment diffÃ©rents

4. **Domain Knowledge Crucial**
   - "Women and children first" â†’ Feature Woman_or_Child
   - Contexte historique guide feature engineering

---

## ğŸ‘¤ Auteur

StÃ©phane TenÃ¨ze
- GitHub: steneze(https://github.com/steneze)
- LinkedIn: StÃ©phane TenÃ¨ze(https://www.linkedin.com/in/st%C3%A9phane-ten%C3%A8ze-b84967/)
- Kaggle: StÃ©phane TenÃ¨ze(https://www.kaggle.com/stphanetenze)

---

## ğŸ™ Remerciements

- Kaggle pour le dataset et la plateforme

---

â­ **Si ce projet vous a Ã©tÃ© utile, n'hÃ©sitez pas Ã  mettre une Ã©toile !** â­